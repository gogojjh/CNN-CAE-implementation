{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP5212 Programming Project 2_CNN Model\n",
      "{'momentum': 0.021, 'learning_rate': 0.01}\n",
      "[INFO] Saving model to ./cnn_model/LR_0.01_MM_0.021\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_tf_random_seed': None, '_model_dir': './cnn_model/LR_0.01_MM_0.021', '_global_id_in_cluster': 0, '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fca36fcae48>, '_is_chief': True, '_task_type': 'worker', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_service': None, '_task_id': 0, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_master': '', '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************[PARAM] FOLD:200 BS:100 LR:0.010000 MM:0.021000\n",
      "Number of folds 200 and Step size 150\n",
      "0 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'accuracy': 0.31125, 'global_step': 150, 'i': 0, 'loss': 3.9312284, 'e': 0}\n",
      "1 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'accuracy': 0.38985, 'global_step': 300, 'i': 1, 'loss': 3.3717484, 'e': 0}\n",
      "2 th fold of training dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d31f7ae8193c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d31f7ae8193c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    294\u001b[0m                             \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m#                         steps=1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                             steps=TRAINING_STEPS/num_fold)\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;31m#                 hooks=[logging_hook])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    544\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1023\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 47\n",
    "\n",
    "FOLD = 200 # default: 200\n",
    "BUFFER_SIZE = 100 # default: 100\n",
    "BATCH_SIZE = 100\n",
    "CRITERIA = 0.001 # default: 0.001\n",
    "LEARNING_RATE = 0.0001\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000 # default: 300000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "EPOH_NUM = 3 # default: 2\n",
    "\n",
    "K = 5 # default: 5\n",
    "\n",
    "CNN_MODEL_PATH = \"./cnn_model/\"\n",
    "MODEL_NAME = \"cnn_model.ckpt\"\n",
    "\n",
    "TASK_TYPE = \"train_cnn\" # change\n",
    "\n",
    "parameter_pair = [[0.01, 0.1], [0.01, 0.01], [0.001, 0.1], [0.001, 0.01], [0.0001, 0.1], [0.0001, 0.01]]    \n",
    "LR = [0.01] # change\n",
    "MM = [0.1] # change\n",
    "    \n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer\n",
    "from sklearn import cross_validation\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.random_seed = 123\n",
    "\n",
    "# TODO: save the process result\n",
    "def save_dict(x, f):\n",
    "    w = csv.writer(open(f, \"w\"))\n",
    "    for i in range(x.shape[0]):\n",
    "        dict = x[i]\n",
    "        for key, val in dict.items():\n",
    "            w.writerow([key, val])\n",
    "\n",
    "# TDOO: SGD with momentum algorithm\n",
    "class MomentOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.1, use_locking=False, name=\"MomentOptimizer\"):\n",
    "        super(MomentOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._momentum = momentum\n",
    "        \n",
    "        self._lr_t = None\n",
    "        self._momentum_t = None\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr, name=\"learning_rate\")\n",
    "        self._alpha_t = ops.convert_to_tensor(self._momentum, name=\"momentum_t\")\n",
    "        self._momentum_t = ops.convert_to_tensor(self._momentum, name=\"momentum_t\")\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for v in var_list:\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        momentum_t = math_ops.cast(self._momentum_t, var.dtype.base_dtype)\n",
    "\n",
    "        eps = 1e-7 #cap for moving average\n",
    "        \n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_t = m.assign(momentum_t * m + lr_t * grad)\n",
    "        var_update = state_ops.assign_sub(var, m_t) \n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t])\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    with tf.variable_scope(\"cnn\") as scope:\n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "#         print(input_layer.shape)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,     \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv1.shape)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=conv1,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "              bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,              \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv2.shape)\n",
    "        \n",
    "        # convolutional layer 3\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            inputs=conv2,\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,            \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv3.shape)\n",
    "\n",
    "        # convolutional layer 4\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "        conv4 = tf.layers.conv2d(\n",
    "            inputs=conv3,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,               \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv4.shape)\n",
    "\n",
    "        # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "        # Output Tensor Shape: [batch_size, 7 * 7 * 64]        \n",
    "        conv4_flat = tf.reshape(conv4, [-1, 7 * 7 * 64])\n",
    "#         print(conv4_flat.shape)\n",
    "            \n",
    "        # dense Layer\n",
    "        dense = tf.layers.dense(\n",
    "            inputs=conv4_flat, \n",
    "            units=1024, \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,             \n",
    "            activation=tf.nn.relu)\n",
    "#         print(dense.shape)\n",
    "        \n",
    "        # output layer\n",
    "        # Input Tensor Shape: [batch_size, 1024]\n",
    "        dropout = tf.layers.dropout(\n",
    "            inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "#         print(dropout.shape)\n",
    "        \n",
    "        # logits Layer\n",
    "        # Input Tensor Shape: [batch_size, 1024]\n",
    "        logits = tf.layers.dense(inputs=dropout, units=OUTPUT_NODE)        \n",
    "#         print(logits.shape)\n",
    "        \n",
    "        # convert raw value from logits layer into two different formats that our model function can return\n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")}\n",
    "            \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)     \n",
    "        tf.summary.scalar(\"cross_entropy_loss\", loss)\n",
    "        \n",
    "        accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "#         tf.identity(accuracy[1], name=\"train_accuracy\")\n",
    "#         tf.summary.scalar(\"prediction_accuracy\", accuracy[1])        \n",
    "        \n",
    "        # 1 train\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = MomentOptimizer(\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                momentum=params[\"momentum\"])\n",
    "#             optimizer = tf.train.GradientDescentOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "    \n",
    "            training_accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "            tf.identity(training_accuracy[1], name=\"training_accuracy\")\n",
    "            tf.summary.scalar(\"accuracy_training_data\", training_accuracy[1])    \n",
    "            \n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        # 2 predict\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)        \n",
    "        \n",
    "        # 3 evaluation\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels=labels, \n",
    "                                            predictions=predictions[\"classes\"])}\n",
    "        \n",
    "#         evaluation_accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "#         tf.identity(evaluation_accuracy[1], name=\"evaluation_accuracy\")        \n",
    "#         tf.summary.scalar(\"accuracy_evaluation_data\", evaluation_accuracy)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, \n",
    "                                          eval_metric_ops=eval_metric_ops)        \n",
    "\n",
    "def main(unused_argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--description\", default=\"COMP5212 Programming Project 2_CNN Model\", \n",
    "                        help=\"increase output verbosity\")\n",
    "    parser.add_argument('--task', default=TASK_TYPE, type=str,\n",
    "                        help='Select the task, train_cnn, test_cnn')\n",
    "    parser.add_argument('--datapath',default=\"../dataset\",type=str, required=False,\n",
    "                        help='Select the path to the data directory')\n",
    "    args = parser.parse_args(args=[])\n",
    "    print(args.description)\n",
    "    datapath = args.datapath\n",
    "      \n",
    "    # read data\n",
    "    file_train = np.load(datapath+\"/data_classifier_train.npz\")\n",
    "    x_train = np.asarray(file_train[\"x_train\"], dtype=np.float32)\n",
    "    y_train = np.asarray(file_train[\"y_train\"], dtype=np.int32)   \n",
    "    file_test = np.load(datapath+\"/data_classifier_test.npz\")\n",
    "    x_test = np.asarray(file_test[\"x_test\"], dtype=np.float32)\n",
    "    y_test = np.asarray(file_test[\"y_test\"], dtype=np.int32)    \n",
    "     \n",
    "    # TODO: make validation data\n",
    "    x_train, x_eval, y_train, y_eval = cross_validation.train_test_split(\n",
    "        x_train, y_train, test_size=0, random_state=0) # default: test_size=1/K\n",
    "    \n",
    "    for L in LR:\n",
    "        for M in MM:\n",
    "            # set cnn model param\n",
    "            t_begin = time.time()\n",
    "            params = {\"learning_rate\": L, \"momentum\": M}\n",
    "            print(params)\n",
    "\n",
    "            CNN_MODEL_PATH_LR_MM = CNN_MODEL_PATH + \"LR_\"+str(L)+\"_MM_\"+str(M)\n",
    "            print(\"[INFO] Saving model to %s\" % CNN_MODEL_PATH_LR_MM)\n",
    "                \n",
    "            # create the estimator\n",
    "            main.emnist_classifier = tf.estimator.Estimator(\n",
    "                model_fn=cnn_model_fn, model_dir=CNN_MODEL_PATH_LR_MM,\n",
    "                params=params)\n",
    "\n",
    "            tensors_to_log = {\"probabilities\": \"softmax_tensor\", \n",
    "                                \"prediction_accuracy_training_data\" : \"training_accuracy\",\n",
    "                                \"prediction_accuracy_evaluation_data\": \"evaluation_accuracy\",\n",
    "                                \"prediction_accuracy\": \"train_accuracy\"}\n",
    "            logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, \n",
    "                                                          every_n_iter=50)        \n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)    \n",
    "            \n",
    "            if args.task == \"train_cnn\":\n",
    "                \n",
    "                # clear the past trained model\n",
    "                if os.path.isdir(CNN_MODEL_PATH_LR_MM):\n",
    "                    shutil.rmtree(CNN_MODEL_PATH_LR_MM,True)                \n",
    "\n",
    "                print(\"*****************[PARAM] FOLD:%d BS:%d LR:%f MM:%f\" % (FOLD, BATCH_SIZE, L, M))\n",
    "                b_exit = False           \n",
    "                num_fold = int(x_train.shape[0] / FOLD); # number of folds 40\n",
    "                print(\"Number of folds %d and Step size %d\" % (num_fold, TRAINING_STEPS/num_fold))\n",
    "                performance_over_time_train = np.array({\"learning_rate\": L, \"momentum\": M})\n",
    "                performance_over_time_eval = np.array({\"learning_rate\": L, \"momentum\": M})              \n",
    "                \n",
    "                for e in range(EPOH_NUM):\n",
    "                    for i in range(num_fold): # 40 times\n",
    "                        print(\"%d th fold of training dataset\" % i)\n",
    "                        x_train_fold = x_train[i*FOLD:min((i+1)*FOLD, x_train.shape[0])]    \n",
    "                        y_train_fold = y_train[i*FOLD:min((i+1)*FOLD, x_train.shape[0])]\n",
    "\n",
    "                        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            x={\"x\": x_train_fold},\n",
    "                            y=y_train_fold,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_epochs=None,\n",
    "                            shuffle=True) \n",
    "\n",
    "                        main.emnist_classifier.train(\n",
    "                            input_fn=train_input_fn,\n",
    "    #                         steps=1000)\n",
    "                            steps=TRAINING_STEPS/num_fold)\n",
    "            #                 hooks=[logging_hook])\n",
    "\n",
    "                        # using training dataset to test the accuracy\n",
    "                        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            x={\"x\": x_train},\n",
    "                            y=y_train,\n",
    "                            num_epochs=1,\n",
    "                            shuffle=False)\n",
    "                        train_results = main.emnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "                        train_results[\"e\"] = e\n",
    "                        train_results[\"i\"] = i\n",
    "                        print(\"[INFO] training performance over times on training data\")\n",
    "                        print(train_results)     \n",
    "                        performance_over_time_train = np.append(performance_over_time_train, train_results)    \n",
    "                        if len(performance_over_time_train) > 2:\n",
    "                            delta_loss = performance_over_time_train[-2][\"loss\"] - train_results[\"loss\"]\n",
    "                            if abs(delta_loss) < CRITERIA:\n",
    "                                b_exit = True\n",
    "\n",
    "                        # using training dataset to test the accuracy\n",
    "#                         eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#                             x={\"x\": x_eval},\n",
    "#                             y=y_eval,\n",
    "#                             num_epochs=1,\n",
    "#                             shuffle=False)\n",
    "#                         eval_results = main.emnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#                         eval_results[\"e\"] = e\n",
    "#                         eval_results[\"i\"] = i\n",
    "#                         print(\"[INFO] training performance over times on evaluation data\")\n",
    "#                         print(eval_results)     \n",
    "#                         performance_over_time_eval = np.append(performance_over_time_eval, eval_results) \n",
    "                    \n",
    "                time_end = time.time() - t_begin\n",
    "                print(\"Training time %f s\" % time_end)   \n",
    "                training_time = {\"training_time(s)\": time_end}\n",
    "                performance_over_time_train = np.append(performance_over_time_train, training_time)\n",
    "                performance_over_time_eval = np.append(performance_over_time_eval, training_time)\n",
    "                str_performance_over_time_train = \"performance_over_time_train\"+\"LR_\"+str(L)+\"_MM_\"+str(M)+\".csv\";\n",
    "                str_performance_over_time_eval = \"performance_over_time_test\"+\"LR_\"+str(L)+\"_MM_\"+str(M)+\".csv\";\n",
    "                save_dict(performance_over_time_train, \"experiment_result/\"+str_performance_over_time_train)\n",
    "                save_dict(performance_over_time_eval, \"experiment_result/\"+str_performance_over_time_eval)\n",
    "                if b_exit:\n",
    "                    break   \n",
    "\n",
    "            elif args.task == \"test_cnn\":\n",
    "                test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"x\": x_test},\n",
    "                    y=y_test,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                test_results = main.emnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "                print(\"Loss %f and Accuracy %f with %d on testing dateset\" \n",
    "                      % (test_results[\"loss\"], test_results[\"accuracy\"], test_results[\"global_step\"]))               \n",
    "                \n",
    "            elif args.task == \"eval_cnn\":\n",
    "                eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"x\": x_eval},\n",
    "                    y=y_eval,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                eval_results = main.emnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "                print(\"Loss %f and Accuracy %f with %d  on evaluation dateset\" \n",
    "                      % (eval_results[\"loss\"], eval_results[\"accuracy\"], eval_results[\"global_step\"]))\n",
    "                \n",
    "            elif args.task == \"test_cnn_with_train\":\n",
    "                test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"x\": x_train},\n",
    "                    y=y_train,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                test_results = main.emnist_classifier.evaluate(input_fn=test_input_fn)\n",
    "                print(\"Loss %f and Accuracy %f with %d on training dateset\" \n",
    "                      % (test_results[\"loss\"], test_results[\"accuracy\"], test_results[\"global_step\"])) \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
