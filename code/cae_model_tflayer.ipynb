{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP5212 Programming Project 2_CAE Model\n",
      "{'learning_rate': 0.005, 'momentum': 0.5}\n",
      "[INFO] Saving model to ./cae_model/LR_0.005_MM_0.5\n",
      "*****************[PARAM] FOLD:200 BS:20 LR:0.005000 MM:0.500000\n",
      "Number of folds 350 and Step size 85\n",
      "0 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.09915291, 'e': 0, 'global_step': 1000, 'i': 0, 'loss': 0.09915323}\n",
      "Epho 0 time 4.300940\n",
      "1 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.08788256, 'e': 0, 'global_step': 2000, 'i': 1, 'loss': 0.08788278}\n",
      "Epho 1 time 9.125138\n",
      "2 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.08127738, 'e': 0, 'global_step': 3000, 'i': 2, 'loss': 0.08127773}\n",
      "Epho 2 time 13.636955\n",
      "3 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.07604822, 'e': 0, 'global_step': 4000, 'i': 3, 'loss': 0.07604853}\n",
      "Epho 3 time 17.840900\n",
      "4 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.05449871, 'e': 0, 'global_step': 5000, 'i': 4, 'loss': 0.05449882}\n",
      "Epho 4 time 22.274787\n",
      "5 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.041060895, 'e': 0, 'global_step': 6000, 'i': 5, 'loss': 0.041060917}\n",
      "Epho 5 time 27.676812\n",
      "6 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.03346312, 'e': 0, 'global_step': 7000, 'i': 6, 'loss': 0.033463154}\n",
      "Epho 6 time 31.907151\n",
      "7 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.029012863, 'e': 0, 'global_step': 8000, 'i': 7, 'loss': 0.02901285}\n",
      "Epho 7 time 36.393299\n",
      "8 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.026928829, 'e': 0, 'global_step': 9000, 'i': 8, 'loss': 0.026928812}\n",
      "Epho 8 time 40.652370\n",
      "9 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.025567457, 'e': 0, 'global_step': 10000, 'i': 9, 'loss': 0.025567459}\n",
      "Epho 9 time 45.668506\n",
      "10 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.024605535, 'e': 0, 'global_step': 11000, 'i': 10, 'loss': 0.024605552}\n",
      "Epho 10 time 50.051085\n",
      "11 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.023859879, 'e': 0, 'global_step': 12000, 'i': 11, 'loss': 0.023859907}\n",
      "Epho 11 time 54.596685\n",
      "12 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.023235604, 'e': 0, 'global_step': 13000, 'i': 12, 'loss': 0.023235641}\n",
      "Epho 12 time 58.848358\n",
      "13 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.022694308, 'e': 0, 'global_step': 14000, 'i': 13, 'loss': 0.022694303}\n",
      "Epho 13 time 64.158599\n",
      "14 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.022214996, 'e': 0, 'global_step': 15000, 'i': 14, 'loss': 0.022215035}\n",
      "Epho 14 time 68.997786\n",
      "15 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.021807136, 'e': 0, 'global_step': 16000, 'i': 15, 'loss': 0.021807188}\n",
      "Epho 15 time 73.281993\n",
      "16 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.021422574, 'e': 0, 'global_step': 17000, 'i': 16, 'loss': 0.021422617}\n",
      "Epho 16 time 78.226956\n",
      "17 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.021122757, 'e': 0, 'global_step': 18000, 'i': 17, 'loss': 0.021122802}\n",
      "Epho 17 time 82.901853\n",
      "18 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.020842362, 'e': 0, 'global_step': 19000, 'i': 18, 'loss': 0.020842405}\n",
      "Epho 18 time 87.375661\n",
      "19 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.020556051, 'e': 0, 'global_step': 20000, 'i': 19, 'loss': 0.020556105}\n",
      "Epho 19 time 92.031577\n",
      "20 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.020348884, 'e': 0, 'global_step': 21000, 'i': 20, 'loss': 0.020348929}\n",
      "Epho 20 time 96.326252\n",
      "21 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.02009523, 'e': 0, 'global_step': 22000, 'i': 21, 'loss': 0.020095294}\n",
      "Epho 21 time 100.678474\n",
      "22 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.019890659, 'e': 0, 'global_step': 23000, 'i': 22, 'loss': 0.019890714}\n",
      "Epho 22 time 105.108089\n",
      "23 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.019713271, 'e': 0, 'global_step': 24000, 'i': 23, 'loss': 0.019713327}\n",
      "Epho 23 time 109.875194\n",
      "24 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01953432, 'e': 0, 'global_step': 25000, 'i': 24, 'loss': 0.019534366}\n",
      "Epho 24 time 114.754425\n",
      "25 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.019365136, 'e': 0, 'global_step': 26000, 'i': 25, 'loss': 0.019365162}\n",
      "Epho 25 time 119.896100\n",
      "26 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.019212928, 'e': 0, 'global_step': 27000, 'i': 26, 'loss': 0.019212976}\n",
      "Epho 26 time 124.251476\n",
      "27 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01904564, 'e': 0, 'global_step': 28000, 'i': 27, 'loss': 0.019045686}\n",
      "Epho 27 time 128.995876\n",
      "28 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018905489, 'e': 0, 'global_step': 29000, 'i': 28, 'loss': 0.018905554}\n",
      "Epho 28 time 133.313933\n",
      "29 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018790917, 'e': 0, 'global_step': 30000, 'i': 29, 'loss': 0.018790964}\n",
      "Epho 29 time 137.825583\n",
      "30 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018654877, 'e': 0, 'global_step': 31000, 'i': 30, 'loss': 0.018654926}\n",
      "Epho 30 time 142.961699\n",
      "31 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018512541, 'e': 0, 'global_step': 32000, 'i': 31, 'loss': 0.018512608}\n",
      "Epho 31 time 146.944628\n",
      "32 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018408176, 'e': 0, 'global_step': 33000, 'i': 32, 'loss': 0.01840823}\n",
      "Epho 32 time 151.522809\n",
      "33 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018287122, 'e': 0, 'global_step': 34000, 'i': 33, 'loss': 0.018287195}\n",
      "Epho 33 time 155.767701\n",
      "34 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01816651, 'e': 0, 'global_step': 35000, 'i': 34, 'loss': 0.018166566}\n",
      "Epho 34 time 159.969158\n",
      "35 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.018064285, 'e': 0, 'global_step': 36000, 'i': 35, 'loss': 0.018064348}\n",
      "Epho 35 time 164.901718\n",
      "36 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017967802, 'e': 0, 'global_step': 37000, 'i': 36, 'loss': 0.017967854}\n",
      "Epho 36 time 169.816137\n",
      "37 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017860407, 'e': 0, 'global_step': 38000, 'i': 37, 'loss': 0.017860461}\n",
      "Epho 37 time 174.404988\n",
      "38 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01776865, 'e': 0, 'global_step': 39000, 'i': 38, 'loss': 0.017768713}\n",
      "Epho 38 time 179.527105\n",
      "39 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01767684, 'e': 0, 'global_step': 40000, 'i': 39, 'loss': 0.017676897}\n",
      "Epho 39 time 184.547964\n",
      "40 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017601961, 'e': 0, 'global_step': 41000, 'i': 40, 'loss': 0.017602032}\n",
      "Epho 40 time 188.644465\n",
      "41 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017494444, 'e': 0, 'global_step': 42000, 'i': 41, 'loss': 0.017494515}\n",
      "Epho 41 time 193.053119\n",
      "42 th fold of training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01741255, 'e': 0, 'global_step': 43000, 'i': 42, 'loss': 0.017412601}\n",
      "Epho 42 time 199.109817\n",
      "43 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017316395, 'e': 0, 'global_step': 44000, 'i': 43, 'loss': 0.017316472}\n",
      "Epho 43 time 204.159394\n",
      "44 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017230017, 'e': 0, 'global_step': 45000, 'i': 44, 'loss': 0.017230071}\n",
      "Epho 44 time 208.775131\n",
      "45 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017165203, 'e': 0, 'global_step': 46000, 'i': 45, 'loss': 0.017165268}\n",
      "Epho 45 time 213.657758\n",
      "46 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017098868, 'e': 0, 'global_step': 47000, 'i': 46, 'loss': 0.017098939}\n",
      "Epho 46 time 218.321217\n",
      "47 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.017014567, 'e': 0, 'global_step': 48000, 'i': 47, 'loss': 0.017014626}\n",
      "Epho 47 time 223.131942\n",
      "48 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016973611, 'e': 0, 'global_step': 49000, 'i': 48, 'loss': 0.016973674}\n",
      "Epho 48 time 227.374305\n",
      "49 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016866462, 'e': 0, 'global_step': 50000, 'i': 49, 'loss': 0.016866531}\n",
      "Epho 49 time 232.182755\n",
      "50 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016798496, 'e': 0, 'global_step': 51000, 'i': 50, 'loss': 0.016798554}\n",
      "Epho 50 time 236.425799\n",
      "51 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016729519, 'e': 0, 'global_step': 52000, 'i': 51, 'loss': 0.016729588}\n",
      "Epho 51 time 240.604792\n",
      "52 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016668567, 'e': 0, 'global_step': 53000, 'i': 52, 'loss': 0.01666862}\n",
      "Epho 52 time 245.282987\n",
      "53 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016598472, 'e': 0, 'global_step': 54000, 'i': 53, 'loss': 0.01659853}\n",
      "Epho 53 time 250.356029\n",
      "54 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01653379, 'e': 0, 'global_step': 55000, 'i': 54, 'loss': 0.016533853}\n",
      "Epho 54 time 255.503989\n",
      "55 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016465819, 'e': 0, 'global_step': 56000, 'i': 55, 'loss': 0.016465882}\n",
      "Epho 55 time 260.164863\n",
      "56 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016397454, 'e': 0, 'global_step': 57000, 'i': 56, 'loss': 0.016397504}\n",
      "Epho 56 time 264.437899\n",
      "57 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01634872, 'e': 0, 'global_step': 58000, 'i': 57, 'loss': 0.016348787}\n",
      "Epho 57 time 268.772881\n",
      "58 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0162801, 'e': 0, 'global_step': 59000, 'i': 58, 'loss': 0.016280143}\n",
      "Epho 58 time 273.742421\n",
      "59 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016226206, 'e': 0, 'global_step': 60000, 'i': 59, 'loss': 0.016226245}\n",
      "Epho 59 time 278.339949\n",
      "60 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016168853, 'e': 0, 'global_step': 61000, 'i': 60, 'loss': 0.016168898}\n",
      "Epho 60 time 282.679679\n",
      "61 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016135987, 'e': 0, 'global_step': 62000, 'i': 61, 'loss': 0.016136039}\n",
      "Epho 61 time 287.976309\n",
      "62 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016076751, 'e': 0, 'global_step': 63000, 'i': 62, 'loss': 0.016076796}\n",
      "Epho 62 time 292.568021\n",
      "63 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.016002772, 'e': 0, 'global_step': 64000, 'i': 63, 'loss': 0.01600283}\n",
      "Epho 63 time 298.063853\n",
      "64 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015969602, 'e': 0, 'global_step': 65000, 'i': 64, 'loss': 0.015969653}\n",
      "Epho 64 time 303.096058\n",
      "65 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015917262, 'e': 0, 'global_step': 66000, 'i': 65, 'loss': 0.01591733}\n",
      "Epho 65 time 307.793619\n",
      "66 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015851505, 'e': 0, 'global_step': 67000, 'i': 66, 'loss': 0.01585156}\n",
      "Epho 66 time 312.266285\n",
      "67 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015795719, 'e': 0, 'global_step': 68000, 'i': 67, 'loss': 0.015795773}\n",
      "Epho 67 time 316.638970\n",
      "68 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015750697, 'e': 0, 'global_step': 69000, 'i': 68, 'loss': 0.015750734}\n",
      "Epho 68 time 321.696716\n",
      "69 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015701585, 'e': 0, 'global_step': 70000, 'i': 69, 'loss': 0.015701635}\n",
      "Epho 69 time 326.443554\n",
      "70 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015666325, 'e': 0, 'global_step': 71000, 'i': 70, 'loss': 0.015666367}\n",
      "Epho 70 time 331.032613\n",
      "71 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015624168, 'e': 0, 'global_step': 72000, 'i': 71, 'loss': 0.015624212}\n",
      "Epho 71 time 335.775115\n",
      "72 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015576753, 'e': 0, 'global_step': 73000, 'i': 72, 'loss': 0.015576792}\n",
      "Epho 72 time 340.581390\n",
      "73 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015518356, 'e': 0, 'global_step': 74000, 'i': 73, 'loss': 0.015518427}\n",
      "Epho 73 time 344.782657\n",
      "74 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015473401, 'e': 0, 'global_step': 75000, 'i': 74, 'loss': 0.015473453}\n",
      "Epho 74 time 349.318575\n",
      "75 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015419244, 'e': 0, 'global_step': 76000, 'i': 75, 'loss': 0.015419282}\n",
      "Epho 75 time 354.413456\n",
      "76 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015368648, 'e': 0, 'global_step': 77000, 'i': 76, 'loss': 0.015368685}\n",
      "Epho 76 time 358.528517\n",
      "77 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015326507, 'e': 0, 'global_step': 78000, 'i': 77, 'loss': 0.015326548}\n",
      "Epho 77 time 363.460436\n",
      "78 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015305567, 'e': 0, 'global_step': 79000, 'i': 78, 'loss': 0.015305601}\n",
      "Epho 78 time 368.487823\n",
      "79 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015265583, 'e': 0, 'global_step': 80000, 'i': 79, 'loss': 0.015265632}\n",
      "Epho 79 time 372.603909\n",
      "80 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015215739, 'e': 0, 'global_step': 81000, 'i': 80, 'loss': 0.01521579}\n",
      "Epho 80 time 376.790050\n",
      "81 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015164435, 'e': 0, 'global_step': 82000, 'i': 81, 'loss': 0.0151644815}\n",
      "Epho 81 time 381.489245\n",
      "82 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015128606, 'e': 0, 'global_step': 83000, 'i': 82, 'loss': 0.015128659}\n",
      "Epho 82 time 386.277308\n",
      "83 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015081488, 'e': 0, 'global_step': 84000, 'i': 83, 'loss': 0.015081533}\n",
      "Epho 83 time 390.368099\n",
      "84 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015043355, 'e': 0, 'global_step': 85000, 'i': 84, 'loss': 0.015043391}\n",
      "Epho 84 time 394.423008\n",
      "85 th fold of training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.015006271, 'e': 0, 'global_step': 86000, 'i': 85, 'loss': 0.015006309}\n",
      "Epho 85 time 398.774045\n",
      "86 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014995299, 'e': 0, 'global_step': 87000, 'i': 86, 'loss': 0.0149953365}\n",
      "Epho 86 time 403.511371\n",
      "87 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014933366, 'e': 0, 'global_step': 88000, 'i': 87, 'loss': 0.0149334}\n",
      "Epho 87 time 408.426150\n",
      "88 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014904689, 'e': 0, 'global_step': 89000, 'i': 88, 'loss': 0.01490475}\n",
      "Epho 88 time 413.705251\n",
      "89 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014868067, 'e': 0, 'global_step': 90000, 'i': 89, 'loss': 0.014868114}\n",
      "Epho 89 time 418.781333\n",
      "90 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014826458, 'e': 0, 'global_step': 91000, 'i': 90, 'loss': 0.014826497}\n",
      "Epho 90 time 424.021643\n",
      "91 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014792921, 'e': 0, 'global_step': 92000, 'i': 91, 'loss': 0.014792955}\n",
      "Epho 91 time 428.602646\n",
      "92 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01475549, 'e': 0, 'global_step': 93000, 'i': 92, 'loss': 0.014755542}\n",
      "Epho 92 time 433.470222\n",
      "93 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0147240665, 'e': 0, 'global_step': 94000, 'i': 93, 'loss': 0.014724114}\n",
      "Epho 93 time 438.443850\n",
      "94 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014671714, 'e': 0, 'global_step': 95000, 'i': 94, 'loss': 0.014671758}\n",
      "Epho 94 time 443.629498\n",
      "95 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014645868, 'e': 0, 'global_step': 96000, 'i': 95, 'loss': 0.014645924}\n",
      "Epho 95 time 448.066971\n",
      "96 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0146102905, 'e': 0, 'global_step': 97000, 'i': 96, 'loss': 0.0146103315}\n",
      "Epho 96 time 452.066485\n",
      "97 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0145646455, 'e': 0, 'global_step': 98000, 'i': 97, 'loss': 0.014564687}\n",
      "Epho 97 time 456.767995\n",
      "98 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014556981, 'e': 0, 'global_step': 99000, 'i': 98, 'loss': 0.014557022}\n",
      "Epho 98 time 461.695049\n",
      "99 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014503094, 'e': 0, 'global_step': 100000, 'i': 99, 'loss': 0.014503132}\n",
      "Epho 99 time 466.209772\n",
      "100 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014464481, 'e': 0, 'global_step': 101000, 'i': 100, 'loss': 0.014464521}\n",
      "Epho 100 time 470.577557\n",
      "101 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014432431, 'e': 0, 'global_step': 102000, 'i': 101, 'loss': 0.014432475}\n",
      "Epho 101 time 475.495884\n",
      "102 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014405847, 'e': 0, 'global_step': 103000, 'i': 102, 'loss': 0.014405884}\n",
      "Epho 102 time 480.066522\n",
      "103 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014371024, 'e': 0, 'global_step': 104000, 'i': 103, 'loss': 0.014371076}\n",
      "Epho 103 time 484.563720\n",
      "104 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014334465, 'e': 0, 'global_step': 105000, 'i': 104, 'loss': 0.014334519}\n",
      "Epho 104 time 488.873687\n",
      "105 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014307443, 'e': 0, 'global_step': 106000, 'i': 105, 'loss': 0.014307478}\n",
      "Epho 105 time 493.639908\n",
      "106 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014289463, 'e': 0, 'global_step': 107000, 'i': 106, 'loss': 0.014289513}\n",
      "Epho 106 time 497.979295\n",
      "107 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01425089, 'e': 0, 'global_step': 108000, 'i': 107, 'loss': 0.014250925}\n",
      "Epho 107 time 502.977465\n",
      "108 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014207028, 'e': 0, 'global_step': 109000, 'i': 108, 'loss': 0.01420708}\n",
      "Epho 108 time 507.019777\n",
      "109 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014177871, 'e': 0, 'global_step': 110000, 'i': 109, 'loss': 0.014177916}\n",
      "Epho 109 time 511.687684\n",
      "110 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014140258, 'e': 0, 'global_step': 111000, 'i': 110, 'loss': 0.014140308}\n",
      "Epho 110 time 516.919568\n",
      "111 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014123012, 'e': 0, 'global_step': 112000, 'i': 111, 'loss': 0.014123059}\n",
      "Epho 111 time 521.772845\n",
      "112 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014079449, 'e': 0, 'global_step': 113000, 'i': 112, 'loss': 0.01407948}\n",
      "Epho 112 time 526.303797\n",
      "113 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.014077645, 'e': 0, 'global_step': 114000, 'i': 113, 'loss': 0.014077704}\n",
      "Epho 113 time 530.660596\n",
      "114 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0140279485, 'e': 0, 'global_step': 115000, 'i': 114, 'loss': 0.014028013}\n",
      "Epho 114 time 534.907043\n",
      "115 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.01399366, 'e': 0, 'global_step': 116000, 'i': 115, 'loss': 0.013993705}\n",
      "Epho 115 time 539.398950\n",
      "116 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013965347, 'e': 0, 'global_step': 117000, 'i': 116, 'loss': 0.013965402}\n",
      "Epho 116 time 543.728299\n",
      "117 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013934503, 'e': 0, 'global_step': 118000, 'i': 117, 'loss': 0.013934556}\n",
      "Epho 117 time 548.359686\n",
      "118 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013896058, 'e': 0, 'global_step': 119000, 'i': 118, 'loss': 0.0138960965}\n",
      "Epho 118 time 553.409516\n",
      "119 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013867519, 'e': 0, 'global_step': 120000, 'i': 119, 'loss': 0.0138675645}\n",
      "Epho 119 time 558.141077\n",
      "120 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013860492, 'e': 0, 'global_step': 121000, 'i': 120, 'loss': 0.013860548}\n",
      "Epho 120 time 562.769341\n",
      "121 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013832178, 'e': 0, 'global_step': 122000, 'i': 121, 'loss': 0.013832239}\n",
      "Epho 121 time 567.057528\n",
      "122 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013785219, 'e': 0, 'global_step': 123000, 'i': 122, 'loss': 0.0137852775}\n",
      "Epho 122 time 571.521220\n",
      "123 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013788254, 'e': 0, 'global_step': 124000, 'i': 123, 'loss': 0.013788297}\n",
      "Epho 123 time 576.666908\n",
      "124 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013739713, 'e': 0, 'global_step': 125000, 'i': 124, 'loss': 0.013739762}\n",
      "Epho 124 time 581.143772\n",
      "125 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0137212705, 'e': 0, 'global_step': 126000, 'i': 125, 'loss': 0.013721308}\n",
      "Epho 125 time 585.499297\n",
      "126 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.0136914095, 'e': 0, 'global_step': 127000, 'i': 126, 'loss': 0.013691453}\n",
      "Epho 126 time 589.886092\n",
      "127 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013668239, 'e': 0, 'global_step': 128000, 'i': 127, 'loss': 0.013668286}\n",
      "Epho 127 time 594.281545\n",
      "128 th fold of training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013641967, 'e': 0, 'global_step': 129000, 'i': 128, 'loss': 0.0136420205}\n",
      "Epho 128 time 598.724411\n",
      "129 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013631391, 'e': 0, 'global_step': 130000, 'i': 129, 'loss': 0.013631439}\n",
      "Epho 129 time 603.759600\n",
      "130 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013600021, 'e': 0, 'global_step': 131000, 'i': 130, 'loss': 0.013600075}\n",
      "Epho 130 time 608.102906\n",
      "131 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013582317, 'e': 0, 'global_step': 132000, 'i': 131, 'loss': 0.013582373}\n",
      "Epho 131 time 613.049334\n",
      "132 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013575534, 'e': 0, 'global_step': 133000, 'i': 132, 'loss': 0.013575578}\n",
      "Epho 132 time 617.867542\n",
      "133 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013541421, 'e': 0, 'global_step': 134000, 'i': 133, 'loss': 0.013541471}\n",
      "Epho 133 time 622.153378\n",
      "134 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013528295, 'e': 0, 'global_step': 135000, 'i': 134, 'loss': 0.013528355}\n",
      "Epho 134 time 627.056691\n",
      "135 th fold of training dataset\n",
      "[INFO] training performance over times on training data\n",
      "{'MSE': 0.013481818, 'e': 0, 'global_step': 136000, 'i': 135, 'loss': 0.013481863}\n",
      "Epho 135 time 631.587097\n",
      "136 th fold of training dataset\n"
     ]
    }
   ],
   "source": [
    "FOLD = 200 # default: 200\n",
    "BUFFER_SIZE = 100 # default: 100\n",
    "BATCH_SIZE = 20\n",
    "CRITERIA = 0.001 # default: 0.001\n",
    "LEARNING_RATE = 0.0001\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000 # default: 300000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "EPOH_NUM = 1 # default: 2\n",
    "\n",
    "K = 5 # default: 5\n",
    "\n",
    "CAE_MODEL_PATH = \"./cae_model/\"\n",
    "MODEL_NAME = \"cae_model.ckpt\"\n",
    "\n",
    "TASK_TYPE = \"train_cae\" # change\n",
    "\n",
    "parameter_pair = [[0.01, 0.2], [0.05, 0.1], [0.001, 0.5], [0.005, 0.3], [0.005, 0.5], [0.008, 0.2]]\n",
    "LR = [0.01] # change\n",
    "MM = [0.2] # change\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer\n",
    "from sklearn import cross_validation\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.random_seed = 123\n",
    "\n",
    "# TODO: save the process result\n",
    "def save_dict(x, f):\n",
    "    w = csv.writer(open(f, \"w\"))\n",
    "    for i in range(x.shape[0]):\n",
    "        dict = x[i]\n",
    "        for key, val in dict.items():\n",
    "            w.writerow([key, val])\n",
    "            \n",
    "def visualize_ae(i, x, features, reconstructed_image):\n",
    "    '''\n",
    "    This might be helpful for visualizing your autoencoder outputs\n",
    "    :param i: index\n",
    "    :param x: original data\n",
    "    :param features: feature maps\n",
    "    :param reconstructed_image: autoencoder output\n",
    "    :return:\n",
    "    '''\n",
    "    plt.figure(0)\n",
    "#     plt.imshow(x[i], cmap=\"gray\")\n",
    "#     plt.figure(1)\n",
    "#     plt.imshow(reconstructed_image[i, :, :, 0], cmap=\"gray\")\n",
    "#     plt.figure(2)\n",
    "#     plt.imshow(np.reshape(features[i, :, :, :], (7, -1), order=\"F\"), cmap=\"gray\",)\n",
    "\n",
    "# TDOO: SGD with momentum algorithm\n",
    "class MomentOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.1, use_locking=False, name=\"MomentOptimizer\"):\n",
    "        super(MomentOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._momentum = momentum\n",
    "        \n",
    "        self._lr_t = None\n",
    "        self._momentum_t = None\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr, name=\"learning_rate\")\n",
    "        self._alpha_t = ops.convert_to_tensor(self._momentum, name=\"momentum_t\")\n",
    "        self._momentum_t = ops.convert_to_tensor(self._momentum, name=\"momentum_t\")\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for v in var_list:\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        momentum_t = math_ops.cast(self._momentum_t, var.dtype.base_dtype)\n",
    "\n",
    "        eps = 1e-7 #cap for moving average\n",
    "        \n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_t = m.assign(momentum_t * m + lr_t*grad)\n",
    "        var_update = state_ops.assign_sub(var, m_t) \n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t])    \n",
    "    \n",
    "def cae_model_fn(features, labels, mode, params):\n",
    "    with tf.variable_scope(\"cae\") as scope:\n",
    "#         print(features[\"x\"].shape)\n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "        input_layer = tf.image.convert_image_dtype(input_layer, tf.float32)\n",
    "#         print(input_layer.shape)\n",
    "\n",
    "        \"\"\"Encoder\"\"\"        \n",
    "        # convolutional layer 1\n",
    "        # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,     \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv1.shape)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=conv1,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,              \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv2.shape)\n",
    "        \n",
    "        # convolutional layer 3\n",
    "        # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            inputs=conv2,\n",
    "            filters=2,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,            \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv3.shape)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"                \n",
    "        # convolutional layer 4\n",
    "        # Input Tensor Shape: [batch_size, 7, 7, 2]\n",
    "        conv4 = tf.layers.conv2d_transpose(\n",
    "            inputs=conv3,\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,               \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv4.shape)\n",
    "\n",
    "        # convolutional layer 5\n",
    "        # Input Tensor Shape: [batch_size, 7, 7, 64]        \n",
    "        conv5 = tf.layers.conv2d_transpose(\n",
    "            inputs=conv4,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,               \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv5.shape)        \n",
    "\n",
    "        # convolutional layer 5\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 32]         \n",
    "        conv6 = tf.layers.conv2d_transpose(\n",
    "            inputs=conv5,\n",
    "            filters=1,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            kernel_regularizer=tf.nn.l2_loss,\n",
    "            bias_regularizer=tf.nn.l2_loss,               \n",
    "            activation=tf.nn.relu)\n",
    "#         print(conv6.shape)        \n",
    "        \n",
    "        features_rec = tf.squeeze(conv6, [3])\n",
    "#         print(features_rec.shape)\n",
    "        \n",
    "        predictions = {\n",
    "            # Generate predictions (for PREDICT and EVAL mode)\n",
    "            \"MSE_training\":tf.metrics.mean_squared_error(labels=labels, predictions=features_rec)\n",
    "        }        \n",
    "        \n",
    "        MSE = tf.metrics.mean_squared_error(labels=labels, predictions=features_rec)\n",
    "        tf.identity(MSE[1], name=\"MSE_training_training\")\n",
    "        tf.summary.scalar(\"MSE_training\",MSE[1])        \n",
    "        \n",
    "        # Loss: Mean squared error\n",
    "        loss = tf.losses.mean_squared_error(labels=labels, predictions=features_rec)\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        \n",
    "        tf.summary.image(\"feature_original\", tensor=input_layer, max_outputs=1)\n",
    "        tf.summary.image(\"feature_reconstruction\", tensor=conv6, max_outputs=1)\n",
    "        tf.summary.image(\"feature_maps_layer_2\", tensor=conv2[:,:,:,0:3], max_outputs=1)\n",
    "        tf.summary.image(\"feature_maps_layer_4\", tensor=conv4[:,:,:,0:3], max_outputs=1)\n",
    "                \n",
    "        # 1 train\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = MomentOptimizer(\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                momentum=params[\"momentum\"])\n",
    "#             optimizer = tf.train.GradientDescentOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        # 2 predict\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)        \n",
    "        \n",
    "        # 3 evaluation\n",
    "        eval_metric_ops = {\n",
    "            \"MSE\": tf.metrics.mean_squared_error(labels=labels, predictions=features_rec)\n",
    "        }\n",
    "    \n",
    "#         visualize_ae(0, input_layer, conv3, conv6);\n",
    "              \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, \n",
    "                                          eval_metric_ops=eval_metric_ops)        \n",
    "\n",
    "def main(unused_argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--description\", default=\"COMP5212 Programming Project 2_CAE Model\", \n",
    "                        help=\"increase output verbosity\")\n",
    "    parser.add_argument('--task', default=TASK_TYPE, type=str,\n",
    "                        help='Select the task, train_cae, test_cae')\n",
    "    parser.add_argument('--datapath',default=\"../dataset\",type=str, required=False,\n",
    "                        help='Select the path to the data directory')\n",
    "    args = parser.parse_args(args=[])\n",
    "    print(args.description)\n",
    "    datapath = args.datapath\n",
    "      \n",
    "    # read data\n",
    "    file_train = np.load(datapath+\"/data_autoencoder_train.npz\")\n",
    "    x_train = np.asarray(file_train[\"x_ae_train\"], dtype=np.float32)/255\n",
    "    file_test = np.load(datapath+\"/data_autoencoder_eval.npz\")\n",
    "    x_test = np.asarray(file_test[\"x_ae_eval\"], dtype=np.float32)/255\n",
    "     \n",
    "    # TODO: make validation data\n",
    "    x_train, x_eval = cross_validation.train_test_split(\n",
    "        x_train, test_size=0, random_state=0) # default: test_size=1/K\n",
    "   \n",
    "    for L in LR:\n",
    "        for M in MM:\n",
    "            # set cnn model param\n",
    "            t_begin = time.time()\n",
    "            params = {\"learning_rate\": L, \"momentum\": M}\n",
    "            print(params)\n",
    "\n",
    "            CAE_MODEL_PATH_LR_MM = CAE_MODEL_PATH + \"LR_\"+str(L)+\"_MM_\"+str(M)\n",
    "            print(\"[INFO] Saving model to %s\" % CAE_MODEL_PATH_LR_MM)\n",
    "                \n",
    "            # create the estimator\n",
    "            main.emnist_feature_extract = tf.estimator.Estimator(\n",
    "                model_fn=cae_model_fn, model_dir=CAE_MODEL_PATH_LR_MM,\n",
    "                params=params)\n",
    "\n",
    "            tensors_to_log = {\"probabilities\": \"softmax_tensor\", \n",
    "                                \"loss\": \"loss\"}\n",
    "            logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, \n",
    "                                                          every_n_iter=50)        \n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)    \n",
    "            \n",
    "            if args.task == \"train_cae\":\n",
    "                \n",
    "                # clear the past trained model # default: comment them\n",
    "                if os.path.isdir(CAE_MODEL_PATH_LR_MM):\n",
    "                    shutil.rmtree(CAE_MODEL_PATH_LR_MM,True)  \n",
    "                    \n",
    "                print(\"*****************[PARAM] FOLD:%d BS:%d LR:%f MM:%f\" % (FOLD, BATCH_SIZE, L, M))\n",
    "                b_exit = False           \n",
    "                num_fold = int(x_train.shape[0] / FOLD); # number of folds 40\n",
    "                print(\"Number of folds %d and Step size %d\" % (num_fold, TRAINING_STEPS/num_fold))\n",
    "                performance_over_time_train = np.array({\"learning_rate\": L, \"momentum\": M})\n",
    "                performance_over_time_eval = np.array({\"learning_rate\": L, \"momentum\": M})              \n",
    "                \n",
    "                for e in range(EPOH_NUM):\n",
    "                    for i in range(num_fold): # 40 times\n",
    "                        print(\"%d th fold of training dataset\" % i)\n",
    "                        x_train_fold = x_train[i*FOLD:min((i+1)*FOLD, x_train.shape[0])]    \n",
    "\n",
    "                        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            x={\"x\": x_train_fold},\n",
    "                            y=x_train_fold,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_epochs=None,\n",
    "                            shuffle=True) \n",
    "\n",
    "                        main.emnist_feature_extract.train(\n",
    "                            input_fn=train_input_fn,\n",
    "                            steps=1000)\n",
    "#                             steps=TRAINING_STEPS/num_fold)\n",
    "            #                 hooks=[logging_hook])\n",
    "\n",
    "                        # using training dataset to test the accuracy\n",
    "                        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            x={\"x\": x_train},\n",
    "                            y=x_train,\n",
    "                            num_epochs=1,\n",
    "                            shuffle=False)\n",
    "                        train_results = main.emnist_feature_extract.evaluate(input_fn=test_input_fn)\n",
    "                        train_results[\"e\"] = e\n",
    "                        train_results[\"i\"] = i\n",
    "                        print(\"[INFO] training performance over times on training data\")\n",
    "                        print(train_results)     \n",
    "                        performance_over_time_train = np.append(performance_over_time_train, train_results)    \n",
    "                        if len(performance_over_time_train) > 2:\n",
    "                            delta_loss = performance_over_time_train[-2][\"loss\"] - train_results[\"loss\"]\n",
    "                            if abs(delta_loss) < CRITERIA:\n",
    "                                b_exit = True\n",
    "                        if train_results[\"loss\"] < 0.01:\n",
    "                            break\n",
    "\n",
    "                        print(\"Epho %d time %f\" %(i, time.time() - t_begin))                            \n",
    "                            \n",
    "                        # using training dataset to test the accuracy\n",
    "                        if x_eval.shape[0] == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            x={\"x\": x_eval},\n",
    "                            y=x_eval,\n",
    "                            num_epochs=1,\n",
    "                            shuffle=False)\n",
    "                        eval_results = main.emnist_feature_extract.evaluate(input_fn=eval_input_fn)\n",
    "                        eval_results[\"e\"] = e\n",
    "                        eval_results[\"i\"] = i\n",
    "                        print(\"[INFO] training performance over times on evaluation data\")\n",
    "                        print(eval_results)    \n",
    "                        \n",
    "#                         performance_over_time_eval = np.append(performance_over_time_eval, eval_results) \n",
    "                    \n",
    "                time_end = time.time() - t_begin\n",
    "                print(\"Training time %f s\" % time_end)   \n",
    "                training_time = {\"training_time(s)\": time_end}\n",
    "#                 performance_over_time_train = np.append(performance_over_time_train, training_time)\n",
    "#                 performance_over_time_eval = np.append(performance_over_time_eval, training_time)\n",
    "#                 str_performance_over_time_train = \"performance_over_time_train\"+\"LR_\"+str(L)+\"_MM_\"+str(M)+\".csv\";\n",
    "#                 str_performance_over_time_eval = \"performance_over_time_test\"+\"LR_\"+str(L)+\"_MM_\"+str(M)+\".csv\";\n",
    "#                 save_dict(performance_over_time_train, \"experiment_result_cae/\"+str_performance_over_time_train)\n",
    "#                 save_dict(performance_over_time_eval, \"experiment_result_cae/\"+str_performance_over_time_eval)\n",
    "#                 if b_exit:\n",
    "#                     break   \n",
    "\n",
    "            elif args.task == \"test_cae\":\n",
    "                test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"x\": x_test},\n",
    "                    y=y_test,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                test_results = main.emnist_feature_extract.evaluate(input_fn=test_input_fn)\n",
    "                print(\"Loss %fith %d on testing dateset\" \n",
    "                      % (test_results[\"loss\"], test_results[\"global_step\"]))               \n",
    "                \n",
    "            elif args.task == \"eval_cae\":\n",
    "                eval_input_fn = tf.estimator.inputs.nu\n",
    "                mpy_input_fn(\n",
    "                    x={\"x\": x_eval},\n",
    "                    y=y_eval,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                eval_results = main.emnist_feature_extract.evaluate(input_fn=eval_input_fn)\n",
    "                print(\"Loss %f %d  on evaluation dateset\" \n",
    "                      % (eval_results[\"loss\"], eval_results[\"global_step\"]))\n",
    "                \n",
    "            elif args.task == \"test_cae_with_train\":\n",
    "                test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"x\": x_train},\n",
    "                    y=y_train,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False)\n",
    "                test_results = main.emnist_feature_extract.evaluate(input_fn=test_input_fn)\n",
    "                print(\"Loss %f with %d on training dateset\" \n",
    "                      % (test_results[\"loss\"], test_results[\"global_step\"])) \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
